#!/usr/bin/env python3
"""
Advanced Ensemble Model Training with Business Logic Constraints
This addresses the core issues: negative modal predictions and unrealistic outputs
"""

import os
import json
import pandas as pd
import numpy as np
import torch
import joblib
import matplotlib.pyplot as plt
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.cluster import KMeans
from sklearn.metrics import mean_absolute_error, r2_score
from statsmodels.tsa.statespace.sarimax import SARIMAX
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import warnings
warnings.filterwarnings('ignore')

# ================= BUSINESS-AWARE MODEL DEFINITIONS ================= #

class BusinessConstrainedMLP(nn.Module):
    """MLP with business logic constraints"""
    def __init__(self, input_size=3, hidden_size=128, output_size=3):
        super().__init__()
        self.backbone = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size//2),
            nn.ReLU(),
            nn.Linear(hidden_size//2, output_size)
        )
        
    def forward(self, x):
        output = self.backbone(x)
        # Apply business constraints
        modal = torch.relu(output[:, 0])  # Modal must be positive
        profit = output[:, 1]  # Profit can be negative
        rugi = torch.relu(output[:, 2])   # Loss must be positive
        return torch.stack([modal, profit, rugi], dim=1)

# ================= DATA PROCESSING ================= #

def load_and_clean_data():
    """Load and clean transaction data with business logic validation"""
    print("üìä Loading and cleaning transaction data...")
    
    data_dir = "generate/dataset/numeric/lanjutan"
    all_data = []
    
    for filename in os.listdir(data_dir):
        if filename.endswith('.json') and filename.startswith('dataset_'):
            filepath = os.path.join(data_dir, filename)
            try:
                with open(filepath, 'r') as f:
                    data = json.load(f)
                    all_data.extend(data)
            except Exception as e:
                print(f"‚ö†Ô∏è Error loading {filename}: {e}")
    
    print(f"‚úÖ Loaded {len(all_data)} raw records")
    
    # Clean and validate data
    cleaned_data = []
    for record in all_data:
        try:
            # Extract and validate fields
            pemasukan = float(record['total_pemasukan'])
            pengeluaran = float(record['total_pengeluaran'])
            modal = float(record['modal_awal'])
            
            # Skip invalid records
            if pemasukan < 0 or pengeluaran < 0 or modal < 0:
                continue
                
            # Calculate proper profit/loss
            net = pemasukan - pengeluaran
            if net > 0:
                profit = net
                rugi = 0
            else:
                profit = 0
                rugi = abs(net)
            
            # Extract hour if available
            try:
                from datetime import datetime
                waktu = datetime.fromisoformat(record['waktu'])
                jam = waktu.hour / 24.0
            except:
                jam = 0.5  # Default to noon
            
            cleaned_data.append({
                'pemasukan': pemasukan,
                'pengeluaran': pengeluaran,
                'jam': jam,
                'modal': modal,
                'profit': profit,
                'rugi': rugi
            })
            
        except Exception as e:
            continue
    
    print(f"‚úÖ Cleaned to {len(cleaned_data)} valid records")
    return cleaned_data

def prepare_features_targets(data):
    """Prepare features and targets with proper scaling"""
    print("üîß Preparing features and targets...")
    
    # Features: [pemasukan, pengeluaran, jam]
    X = np.array([[d['pemasukan'], d['pengeluaran'], d['jam']] for d in data])
    
    # Targets: [modal, profit, rugi]
    y = np.array([[d['modal'], d['profit'], d['rugi']] for d in data])
    
    # Use RobustScaler to handle outliers better
    scaler_x = RobustScaler()
    scaler_y = RobustScaler()
    
    X_scaled = scaler_x.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)
    
    print(f"‚úÖ Feature shapes: X={X_scaled.shape}, y={y_scaled.shape}")
    print(f"‚úÖ Feature ranges: X=[{X_scaled.min():.3f}, {X_scaled.max():.3f}]")
    print(f"‚úÖ Target ranges: y=[{y_scaled.min():.3f}, {y_scaled.max():.3f}]")
    
    return X_scaled, y_scaled, scaler_x, scaler_y

# ================= TRAINING FUNCTIONS ================= #

def train_constrained_mlp(X_train, y_train, X_test, y_test):
    """Train business-constrained MLP"""
    print("üß† Training Business-Constrained MLP...")
    
    model = BusinessConstrainedMLP(input_size=3, hidden_size=128, output_size=3)
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    criterion = nn.MSELoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)
    
    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), 
                                 torch.tensor(y_train, dtype=torch.float32))
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    
    best_loss = float('inf')
    patience = 30
    patience_counter = 0
    
    train_losses = []
    val_losses = []
    
    for epoch in range(300):
        model.train()
        epoch_loss = 0.0
        
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            output = model(batch_x)
            loss = criterion(output, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            epoch_loss += loss.item()
        
        # Validation
        model.eval()
        with torch.no_grad():
            val_output = model(torch.tensor(X_test, dtype=torch.float32))
            val_loss = criterion(val_output, torch.tensor(y_test, dtype=torch.float32)).item()
        
        train_losses.append(epoch_loss / len(train_loader))
        val_losses.append(val_loss)
        scheduler.step(val_loss)
        
        if val_loss < best_loss:
            best_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), "ensemble/models/mlp_model.pth")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch}")
                break
        
        if epoch % 50 == 0:
            print(f"Epoch {epoch}: Train Loss {train_losses[-1]:.6f}, Val Loss {val_loss:.6f}")
    
    # Plot training curves
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('MLP Training Progress')
    plt.legend()
    plt.grid(True)
    plt.savefig('ensemble/models/mlp_training_curve.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"‚úÖ MLP training completed. Best validation loss: {best_loss:.6f}")
    return model

def train_robust_tree_models(X_train, y_train, X_test, y_test):
    """Train robust tree models with hyperparameter tuning"""
    print("üå≥ Training Robust Tree Models...")
    
    # Decision Tree with better parameters
    tree = DecisionTreeRegressor(
        random_state=42,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        max_features='sqrt'
    )
    tree.fit(X_train, y_train)
    
    # Random Forest with optimized parameters
    rf = RandomForestRegressor(
        n_estimators=200,
        random_state=42,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        max_features='sqrt',
        n_jobs=-1
    )
    rf.fit(X_train, y_train)
    
    # Evaluate models
    tree_pred = tree.predict(X_test)
    rf_pred = rf.predict(X_test)
    
    tree_mae = mean_absolute_error(y_test, tree_pred)
    rf_mae = mean_absolute_error(y_test, rf_pred)
    tree_r2 = r2_score(y_test, tree_pred)
    rf_r2 = r2_score(y_test, rf_pred)
    
    print(f"‚úÖ Decision Tree - MAE: {tree_mae:.4f}, R¬≤: {tree_r2:.4f}")
    print(f"‚úÖ Random Forest - MAE: {rf_mae:.4f}, R¬≤: {rf_r2:.4f}")
    
    # Save models
    joblib.dump(tree, "ensemble/models/tree_model.pkl")
    joblib.dump(rf, "ensemble/models/rf_model.pkl")
    
    return tree, rf

def train_arimax_with_plots(sales_data=None):
    """Train ARIMAX model with visualization"""
    print("üìä Training ARIMAX Model with Visualization...")
    
    try:
        if sales_data is None:
            # Create synthetic sales data based on business patterns
            np.random.seed(42)
            dates = pd.date_range('2023-01-01', periods=365, freq='D')
            
            # Create realistic sales pattern
            trend = np.linspace(1000, 1200, 365)
            seasonal = 200 * np.sin(2 * np.pi * np.arange(365) / 7)  # Weekly pattern
            noise = np.random.normal(0, 50, 365)
            sales = trend + seasonal + noise
            
            # Create exogenous variables
            promo = np.random.binomial(1, 0.3, 365)  # 30% promo days
            holiday = np.random.binomial(1, 0.1, 365)  # 10% holidays
            
            sales_df = pd.DataFrame({
                'date': dates,
                'sales': sales,
                'promo': promo,
                'holiday': holiday
            })
            sales_df.set_index('date', inplace=True)
        
        # Split data
        train_size = int(0.8 * len(sales_df))
        train_data = sales_df[:train_size]
        test_data = sales_df[train_size:]
        
        # Train ARIMAX model
        model = SARIMAX(
            train_data['sales'],
            exog=train_data[['promo', 'holiday']],
            order=(1, 1, 1),
            seasonal_order=(1, 1, 1, 7)
        )
        fitted_model = model.fit(disp=False, maxiter=200)
        
        # Make predictions
        forecast = fitted_model.forecast(
            steps=len(test_data),
            exog=test_data[['promo', 'holiday']]
        )
        
        # Create visualization
        plt.figure(figsize=(15, 10))
        
        # Plot 1: Sales data and forecast
        plt.subplot(2, 2, 1)
        plt.plot(train_data.index, train_data['sales'], label='Training Data', color='blue')
        plt.plot(test_data.index, test_data['sales'], label='Actual Test Data', color='green')
        plt.plot(test_data.index, forecast, label='Forecast', color='red', linestyle='--')
        plt.title('ARIMAX Sales Forecast')
        plt.xlabel('Date')
        plt.ylabel('Sales')
        plt.legend()
        plt.grid(True)
        
        # Plot 2: Residuals
        plt.subplot(2, 2, 2)
        residuals = fitted_model.resid
        plt.plot(residuals)
        plt.title('Model Residuals')
        plt.xlabel('Time')
        plt.ylabel('Residuals')
        plt.grid(True)
        
        # Plot 3: Forecast vs Actual
        plt.subplot(2, 2, 3)
        plt.scatter(test_data['sales'], forecast, alpha=0.6)
        plt.plot([test_data['sales'].min(), test_data['sales'].max()], 
                [test_data['sales'].min(), test_data['sales'].max()], 'r--')
        plt.xlabel('Actual Sales')
        plt.ylabel('Predicted Sales')
        plt.title('Forecast vs Actual')
        plt.grid(True)
        
        # Plot 4: Feature importance (exogenous variables effect)
        plt.subplot(2, 2, 4)
        exog_effects = fitted_model.params[-2:]  # Last 2 params are exog coefficients
        plt.bar(['Promo Effect', 'Holiday Effect'], exog_effects)
        plt.title('Exogenous Variables Impact')
        plt.ylabel('Coefficient Value')
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('ensemble/models/arimax_analysis.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        # Calculate metrics
        mae = mean_absolute_error(test_data['sales'], forecast)
        print(f"‚úÖ ARIMAX Model - MAE: {mae:.2f}")
        
        # Save model and data
        joblib.dump(fitted_model, "ensemble/models/arimax_model.pkl")
        sales_df.to_csv("ensemble/data/sales_data.csv")
        
        print("‚úÖ ARIMAX model trained and visualizations saved")
        return fitted_model
        
    except Exception as e:
        print(f"‚ö†Ô∏è ARIMAX training failed: {e}")
        # Create dummy model
        class DummyARIMAX:
            def forecast(self, steps=1, exog=None):
                return np.array([7500.0] * steps)
        
        dummy_model = DummyARIMAX()
        joblib.dump(dummy_model, "ensemble/models/arimax_model.pkl")
        return dummy_model

def train_smart_kmeans(X_train):
    """Train KMeans with optimal number of clusters"""
    print("üéØ Training Smart KMeans Clustering...")
    
    # Use only financial features for clustering
    X_cluster = X_train[:, :2]  # pemasukan, pengeluaran
    
    # Find optimal number of clusters using elbow method
    inertias = []
    K_range = range(2, 8)
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X_cluster)
        inertias.append(kmeans.inertia_)
    
    # Plot elbow curve
    plt.figure(figsize=(8, 6))
    plt.plot(K_range, inertias, 'bo-')
    plt.xlabel('Number of Clusters (k)')
    plt.ylabel('Inertia')
    plt.title('Elbow Method for Optimal k')
    plt.grid(True)
    plt.savefig('ensemble/models/kmeans_elbow.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    # Use k=3 for business categories: small, medium, large
    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
    kmeans.fit(X_cluster)
    
    joblib.dump(kmeans, "ensemble/models/kmeans_model.pkl")
    print("‚úÖ KMeans clustering completed")
    return kmeans

# ================= MAIN TRAINING PIPELINE ================= #

def main():
    """Advanced training pipeline with business constraints"""
    print("üöÄ Advanced Ensemble Model Training with Business Logic")
    print("=" * 70)
    
    # Create directories
    os.makedirs("ensemble/models", exist_ok=True)
    os.makedirs("ensemble/data", exist_ok=True)
    
    # Load and clean data
    data = load_and_clean_data()
    X_scaled, y_scaled, scaler_x, scaler_y = prepare_features_targets(data)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_scaled, test_size=0.2, random_state=42
    )
    
    # Save scalers
    joblib.dump(scaler_x, "ensemble/data/scaler_x.pkl")
    joblib.dump(scaler_y, "ensemble/data/scaler_y.pkl")
    
    # Save processed data
    feature_names = ['pemasukan', 'pengeluaran', 'jam']
    target_names = ['modal', 'profit', 'rugi']
    
    pd.DataFrame(X_train, columns=feature_names).to_csv("ensemble/data/X_scaled.csv", index=False)
    pd.DataFrame(y_train, columns=target_names).to_csv("ensemble/data/y_scaled.csv", index=False)
    
    print("‚úÖ Data preprocessing completed")
    
    # Train all models
    print("\nüéØ Training Advanced Models...")
    print("-" * 50)
    
    # 1. Train constrained MLP
    mlp_model = train_constrained_mlp(X_train, y_train, X_test, y_test)
    
    # 2. Train robust tree models
    tree_model, rf_model = train_robust_tree_models(X_train, y_train, X_test, y_test)
    
    # 3. Train ARIMAX with visualization
    arimax_model = train_arimax_with_plots()
    
    # 4. Train smart KMeans
    kmeans_model = train_smart_kmeans(X_train)
    
    print("\nüéâ Advanced Training Completed!")
    print("=" * 70)
    print("üìÅ Models saved in: ensemble/models/")
    print("üìÅ Data saved in: ensemble/data/")
    print("üìä Visualizations saved as PNG files")
    print("\n‚úÖ Ready for business-aware predictions!")

if __name__ == "__main__":
    main()
